name: Generate and Cache Large Files

on: 
  workflow_dispatch:

jobs:
  generate-and-cache:
    runs-on: ubuntu-latest
    strategy:
      # This creates 12 separate jobs, one for each number.
      # Each job runs on a fresh machine, preventing disk space issues.
      matrix:
        file_index: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
      # Optional: Remove this line if you want them to run in parallel (faster).
      # max-parallel: 1 

    steps:
      - name: Create Output Directory
        run: mkdir -p generated-data

      - name: Generate Single File (1GB)
        run: |
          filename="generated-data/file_${{ matrix.file_index }}.txt"
          echo "Generating $filename..."
          
          # Generate exactly one 1GB file for this job
          dd if=/dev/random of="$filename" bs=1M count=1024 status=none
          
          echo "File generated:"
          ls -lh "$filename"

      - name: Cache Single File
        # We use 'save' specifically since we are creating new data
        uses: actions/cache/save@v4
        id: cache-large-file
        with:
          path: generated-data/file_${{ matrix.file_index }}.txt
          # Unique key for this specific file
          key: large-file-cache-${{ github.run_id }}-${{ matrix.file_index }}
